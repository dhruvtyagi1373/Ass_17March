{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75fe21e7-1572-4b5c-ae56-313ed5b2b2fc",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a37aff9-5a15-45e0-bc8d-b5801012e1df",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a particular value in a specific field or observation. They can occur due to various reasons such as data collection errors, data corruption, or deliberate omission.\n",
    "Handling missing values is crucial for several reasons:\n",
    "- Prevent Biased Results: Missing values can skew the results and analysis, leading to biased interpretations and incorrect conclusions.\n",
    "- Maintain Data Quality: To ensure the quality and reliability of the data for analysis and modeling.\n",
    "- Algorithmsâ€™ Compatibility: Many machine learning algorithms cannot handle missing values and might throw errors during training or testing.\n",
    "\n",
    "Several algorithms are robust to missing values, including:\n",
    "- Decision Trees: They can work well with missing values by finding the best split based on available data in the feature.\n",
    "- Random Forest: Being an ensemble method built on decision trees, it can handle missing values effectively.\n",
    "- K-Nearest Neighbors (KNN): KNN algorithms can handle missing values by ignoring the missing attributes when calculating distances between instances.\n",
    "- Naive Bayes: This probabilistic algorithm can handle missing data by ignoring the missing values during probability calculations.\n",
    "\n",
    "Handling missing values can be done through techniques like:\n",
    "\n",
    "1. Imputation: Filling in missing values with mean, median, mode, or more sophisticated methods based on the data distribution.\n",
    "2. Deletion: Removing observations or features with missing values (if the proportion of missing data is small).\n",
    "3. Prediction Models: Using machine learning algorithms to predict missing values based on other available data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d2992-30a1-40d5-9dbd-70654f5bba40",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2eaf0-a703-4b0a-b898-59040d920619",
   "metadata": {},
   "source": [
    "1. Mean/Median/Mode Imputation:\n",
    "Filling missing values with the mean (for continuous numerical data), median, or mode (for categorical data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63b6f2b-b728-4990-a46e-529767206622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0     a\n",
      "1  2.0     b\n",
      "2  3.0  None\n",
      "3  4.0     a\n",
      "4  5.0     c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': ['a', 'b', None, 'a', 'c']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values in column 'A' with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['A'] = imputer.fit_transform(df[['A']])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712f39e-6c80-4ff5-8c06-6d395ade44d2",
   "metadata": {},
   "source": [
    "2. Forward Fill/Backward Fill:\n",
    "Propagating non-missing values forward or backward to fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077406dd-6e4b-4c10-956c-685f29ae9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A  B\n",
      "0  1.0  a\n",
      "1  2.0  b\n",
      "2  3.0  b\n",
      "3  4.0  a\n",
      "4  5.0  c\n"
     ]
    }
   ],
   "source": [
    "# Forward fill missing values in DataFrame\n",
    "df_ffill = df.ffill()\n",
    "print(df_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20652c3b-fb53-44e0-9a64-423b102cacb6",
   "metadata": {},
   "source": [
    "3. K-Nearest Neighbors (KNN) Imputation:\n",
    "Using the values of the nearest neighbors to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe641db-454e-40d4-aceb-ad9566b85c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0  10.0\n",
      "1  2.0  25.0\n",
      "2  2.5  30.0\n",
      "3  4.0  40.0\n",
      "4  5.0  50.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [10, None, 30, 40, 50]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values using KNN\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ad735-5467-4ce1-8f17-6c216b5ea9fd",
   "metadata": {},
   "source": [
    "4. Deletion (Dropna):\n",
    "Removing rows or columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cccda-4a33-4111-adc6-84b92bb96aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with any missing values\n",
    "df_dropna = df.dropna()\n",
    "print(df_dropna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7a5ea-7654-469d-b033-09adeee3363b",
   "metadata": {},
   "source": [
    "5. Prediction Models:\n",
    "Using machine learning models to predict missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c22c7f-779e-42c3-a5e6-c80a295bfd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0  10.0\n",
      "1  2.0  20.0\n",
      "2  NaN  30.0\n",
      "3  4.0  40.0\n",
      "4  5.0  50.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [10, None, 30, 40, 50]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate rows with missing values and without missing values\n",
    "df_missing = df[df['B'].isnull()]\n",
    "df_not_missing = df.dropna()\n",
    "\n",
    "# Train a model to predict missing values\n",
    "model = RandomForestRegressor()\n",
    "model.fit(df_not_missing[['A']], df_not_missing['B'])\n",
    "predicted_values = model.predict(df_missing[['A']])\n",
    "df.loc[df['B'].isnull(), 'B'] = predicted_values\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62584364-b444-48db-af82-dc360bf97a88",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98f51b-5deb-496f-b84c-7799eeb99e7c",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes within a dataset is skewed, meaning that one or more classes have significantly more or fewer instances than the others. For instance, in a binary classification problem, if one class has 95% of the data while the other has only 5%, it's considered imbalanced.\n",
    "\n",
    "When imbalanced data is not handled, several issues can arise:\n",
    "\n",
    "- Biased Model Performance: Algorithms trained on imbalanced data tend to be biased towards the majority class. They might classify everything as the majority class to maximize accuracy, completely ignoring the minority class.\n",
    "- Poor Generalization: The model may not generalize well to new data, especially if the real-world distribution differs from the training data. It might fail to recognize or predict instances from the minority class.\n",
    "- Misleading Evaluation Metrics: Common metrics like accuracy can be misleading in imbalanced settings. Even a high accuracy score can be achieved by simply predicting the majority class.\n",
    "- Loss of Information: Ignoring the minority class can result in losing critical information or patterns that could be present in that class.\n",
    "\n",
    "To handle imbalanced data, various techniques can be employed:\n",
    "1. Resampling: Oversampling the minority class or undersampling the majority class to balance the class distribution.\n",
    "2. Generating Synthetic Samples: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) create synthetic examples of the minority class.\n",
    "3. Cost-Sensitive Learning: Adjusting the misclassification cost to penalize errors in the minority class more than the majority class.\n",
    "4. Ensemble Methods: Using ensemble algorithms like Random Forest or Gradient Boosting which inherently handle imbalanced data better than single models.\n",
    "5. Different Evaluation Metrics: Using metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) that are more informative in imbalanced settings.\n",
    "\n",
    "Handling imbalanced data is crucial to ensure that machine learning models learn from all classes adequately and make predictions that are inclusive of all potential outcomes, not just the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209a756-901e-47d1-adc6-c9a507dfc355",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac46ec-d8cc-4f3d-bf5e-975e3334dc3f",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address imbalanced datasets by adjusting the class distribution.\n",
    "\n",
    "Up-sampling:\n",
    "Up-sampling involves increasing the number of instances in the minority class to balance the dataset. This is typically done by duplicating or creating synthetic examples of the minority class.\n",
    "\n",
    "Example of up-sampling:\n",
    "Consider a dataset with two classes: Class A (majority) has 100 instances, and Class B (minority) has 20 instances. To balance the dataset, you might create additional synthetic instances for Class B to match the number of instances in Class A, making it 100 instances as well.\n",
    "\n",
    "Down-sampling:\n",
    "Down-sampling involves reducing the number of instances in the majority class to balance the dataset. This is typically done by randomly removing instances from the majority class.\n",
    "\n",
    "Example of down-sampling:\n",
    "In the same dataset with Class A (majority) having 100 instances and Class B (minority) having 20 instances, down-sampling would randomly remove instances from Class A to match the number of instances in Class B.\n",
    "\n",
    "When are they required?\n",
    "\n",
    "- Up-sampling is often used when the amount of data in the minority class is limited, and creating synthetic examples or duplicating existing ones can help improve the learning of the model on that class.\n",
    "- Down-sampling is employed when the majority class has a significant number of instances, and removing some of them can help rebalance the dataset, reducing the dominance of the majority class.\n",
    "\n",
    "Deciding whether to up-sample or down-sample depends on the specific dataset and problem context. Up-sampling may be more effective when the dataset is small or when information in the minority class is crucial. Down-sampling may be preferred when the majority class is excessively large, leading to dominance in the learning process. Often, a combination of both techniques or more advanced methods like SMOTE (Synthetic Minority Over-sampling Technique) can be employed for better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6102f6f-54d4-4354-aae6-680964b25167",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae1820-efc4-487d-b995-f6a33e92d538",
   "metadata": {},
   "source": [
    "Data Augmentation involves creating new, synthetic data from existing data while preserving its original meaning. It's commonly used in machine learning to expand a dataset by applying various transformations, such as rotation, scaling, cropping, or adding noise, to existing data points. This technique helps in diversifying the dataset, reducing overfitting, and improving the model's generalization.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific method for data augmentation designed to address imbalanced datasets, particularly in the context of classification problems. It focuses on the minority class by generating synthetic examples to balance the class distribution.\n",
    "\n",
    "The process involves these steps:\n",
    "- Identifying Minority Class Instances: For a binary classification problem, the minority class is the class with fewer instances.\n",
    "- Selecting a Minority Instance: SMOTE chooses a minority class instance and finds its k-nearest neighbors in the feature space.\n",
    "- Generating Synthetic Examples: New instances are created by interpolating between the chosen instance and its nearest neighbors. This interpolation is done by selecting random points along the line segments joining the chosen instance to its neighbors. These points become the synthetic instances.\n",
    "- Repeating the Process: This process is repeated until the desired balance between the minority and majority classes is achieved.\n",
    "\n",
    "SMOTE helps to overcome the imbalance issue in the dataset without duplicating existing instances. By creating synthetic instances, it provides the model with more information about the minority class, thus reducing the risk of biased classification towards the majority class.\n",
    "\n",
    "For example, if you have a dataset for credit card fraud detection where fraudulent transactions (minority class) are rare compared to legitimate ones (majority class), applying SMOTE can help generate synthetic instances of fraudulent transactions based on the existing minority samples, making the dataset more balanced and aiding the model in learning the patterns of fraudulent behavior more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c5a6d-d78c-4ac4-a342-4bca6075ea6c",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23ae2c-3455-4ad6-a078-657e1d48737f",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly differ from other observations in a dataset. They can exist in numerical data as unusually high or low values and in categorical data as unusual categories that are rare or unexpected compared to the rest of the data.\n",
    "\n",
    "It's crucial to handle outliers for several reasons:\n",
    "- Impact on Statistical Analysis: Outliers can heavily skew statistical measures such as the mean and standard deviation, affecting the overall distribution and making it less representative of the majority of the data.\n",
    "- Model Performance: Outliers can distort the results of statistical models by introducing noise or bias. Models can end up overemphasizing the significance of the outliers, affecting their predictive performance.\n",
    "- Misleading Conclusions: Outliers can mislead analysts into drawing incorrect conclusions about the dataset and the relationships between variables.\n",
    "- Influence on Machine Learning Models: Algorithms like linear regression, which are sensitive to outliers, can be heavily impacted, leading to inaccurate predictions.\n",
    "\n",
    "Handling outliers involves various techniques:\n",
    "\n",
    "- Identification: Use statistical methods like the z-score, IQR (interquartile range), or visualization tools to identify outliers in the dataset.\n",
    "- Transformation: Techniques like log transformation, square root transformation, or winsorization can be applied to modify the data distribution and reduce the impact of outliers.\n",
    "- Deletion: Removing outliers from the dataset, especially if they are due to measurement errors or anomalies.\n",
    "- Imputation: Replacing outliers with more reasonable values based on domain knowledge or statistical techniques.\n",
    "- Robust Models: Using models that are inherently robust to outliers, such as decision trees or random forests.\n",
    "\n",
    "Handling outliers is crucial to ensure the integrity of analyses, model training, and the accuracy of predictive models. It helps in creating more reliable and robust representations of the underlying data, allowing for better insights and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb08e28-7434-4975-a2b2-863f36489dc3",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1edf0ce-b823-4e96-a7e2-51170544ee97",
   "metadata": {},
   "source": [
    "Handling missing data is crucial for accurate analysis. Here are some techniques:\n",
    "1. Data Imputation: Replace missing values with a statistical estimate like mean, median, or mode of the available data.\n",
    "2. Deletion: Remove rows or columns with missing data. This method can be useful if the missing data is minimal and won't significantly affect the analysis.\n",
    "3. Prediction Models: Use machine learning algorithms to predict missing values based on other features in the dataset.\n",
    "4. K-Nearest Neighbors (KNN): Impute missing values based on similarity to other data points.\n",
    "5. Mean Substitution: Replace missing values with the mean of the non-missing values in the column.\n",
    "6. Forward or Backward Filling: Propagate the last known value forward or backward to fill missing data.\n",
    "7. Domain Knowledge: Use subject matter expertise to infer or estimate missing values based on the context and understanding of the data.\n",
    "8. Multiple Imputation: Create multiple versions of the dataset with different imputed values to capture uncertainty in missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f898ca8-8918-456d-88ac-f1ce8cf187ce",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824609e-9130-49a6-bdee-f6f6387ef90d",
   "metadata": {},
   "source": [
    "Determining the nature of missing data is crucial for accurate analysis. Here are some strategies to discern whether the missing data is random or follows a pattern:\n",
    "- Statistical Tests: Utilize statistical tests to evaluate the randomness of missing data. For instance, the MCAR (Missing Completely at Random), MAR (Missing at Random), or MNAR (Missing Not at Random) tests help determine the relationship between the missing data and the observed data.\n",
    "- Visualization: Plotting data can often reveal patterns. Use tools like heatmaps or missing data matrices to visually inspect where the missing values occur. Look for correlations between missing values and other variables.\n",
    "- Summary Statistics: Compare summary statistics (mean, median, standard deviation) between data with missing values and the complete dataset. Differences might suggest patterns in missing data.\n",
    "- Correlation Analysis: Analyze correlations between variables to check if missingness is related to other variables in the dataset.\n",
    "- Imputation Test: Perform imputation using different methods and compare results. If the imputation results vary significantly, it could suggest a pattern in the missing data.\n",
    "- Domain Knowledge and Expert Consultation: Domain knowledge might provide insights into why certain data is missing. Consulting experts in the field can shed light on potential reasons for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c87cd-aa57-45ef-a3a1-c26690006f17",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3523456-d330-4ede-9b96-59d506a779d0",
   "metadata": {},
   "source": [
    "Dealing with imbalanced datasets in medical diagnosis is common and demands specific strategies to ensure accurate model performance. Here are some techniques:\n",
    "\n",
    "Resampling Methods:\n",
    "Oversampling: Increase the number of instances in the minority class by duplicating samples or generating synthetic examples.\n",
    "Undersampling: Reduce the number of instances in the majority class to balance it with the minority class.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic samples for the minority class to create a more balanced dataset.\n",
    "\n",
    "Performance Metrics:\n",
    "Avoid using accuracy as the sole metric. Utilize metrics like precision, recall, F1-score, and area under the ROC curve (AUC-ROC) that are more sensitive to imbalanced data.\n",
    "Precision focuses on the accuracy of the positive predictions, while recall measures the model's ability to capture the positives. F1-score balances both precision and recal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9416e-d0df-456d-ac85-257df1f4541c",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb96ed5-7ffc-47a6-b02d-863e05cb697d",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset in customer satisfaction estimation, down-sampling the majority class can help balance the data. Here are techniques you can employ:\n",
    "\n",
    "Random Under-Sampling:\n",
    "Randomly remove instances from the majority class to balance the dataset. While straightforward, it might lead to loss of information.\n",
    "\n",
    "Cluster-Based Undersampling:\n",
    "Use clustering techniques to group data points in the majority class and then remove samples from these clusters to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de851327-aef5-42d1-b775-82b184983a7f",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e28ce-7b00-4ebf-ae0a-f291d59c6c51",
   "metadata": {},
   "source": [
    "When dealing with a dataset where occurrences of a rare event are significantly lower, up-sampling the minority class can help balance the data. Here are several methods to consider:\n",
    "\n",
    "Random Over-Sampling:\n",
    "Duplicate instances from the minority class randomly to balance the dataset. However, this might lead to overfitting.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-Sampling Technique):\n",
    "Generate synthetic examples for the minority class by creating new instances based on the existing ones, thus expanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fa946-3d57-48e3-8043-46862bc4e363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6617fd7-5811-41d6-8bd9-5d241f951b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
